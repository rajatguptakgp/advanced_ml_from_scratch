{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "feature_cols = dataset['feature_names']\n",
    "df = pd.DataFrame(dataset['data'], columns = feature_cols)\n",
    "df['target'] = dataset['target']\n",
    "X = df[feature_cols].values\n",
    "y = df['target'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 3000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_score = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Interpretation of AUC\n",
    "\n",
    "AUC is defined as probability that a random positive example is positively scored higher than a random negative example.\n",
    "Let there be total of $M$ examples, out of which $P$ are positive and $N$ are negative examples. Hence, $M = P + N$.\n",
    "\n",
    "Fit a classification model and get probabilities/confidences of predicting positive label. Count all cases out of $P * N$, where confidence for $p$ is greater than $n$, where $p$ is one of $P$ examples and $n$ is one of $N$ examples. Let that count be $C$.\n",
    "\n",
    "AUC is given by $ C / (P * N) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def probabilistic_AUC(y, y_score):\n",
    "    # getting indexes for positive and negative examples\n",
    "    pos_idxs = np.nonzero(y == 1)[0]\n",
    "    neg_idxs = np.nonzero(y == 0)[0]\n",
    "\n",
    "    # counting instances when positive example was scored higher than negative example\n",
    "    count = 0\n",
    "    for pos_idx in pos_idxs:\n",
    "        for neg_idx in neg_idxs:\n",
    "            if y_score[:, 1][pos_idx] > y_score[:, 1][neg_idx]:\n",
    "                count += 1\n",
    "\n",
    "    # total combinations - n_pos * n_neg\n",
    "    prob = count / (len(pos_idxs) * len(neg_idxs))\n",
    "    return prob\n",
    "\n",
    "probabilistic_AUC(y, y_score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947412927435124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, y_score[:, 1], pos_label = 1)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class AUC (MAUC)\n",
    "\n",
    "There are two approaches to calculate AUC in multi-class problems (number of classes >= 3):\n",
    "1. **One v/s Rest (OVR):** \n",
    "    1. Considering one class and rest of classes as another class. Modify true labels and probabilities accordingly and get AUC. \n",
    "    2. Take average of all combinations.\n",
    "    3. Number of models is equal to number of classes\n",
    "2. **One v/s One (OVO):** \n",
    "    1. Consider any two classes. Fit a binary classifier and get AUC. \n",
    "    2. Take average of all combinations.\n",
    "    3. Number of models is equal to $K * (K - 1) / 2$, where $K$ is number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 3\n",
    "X, y = make_classification(n_samples = 1000, n_features = 10, n_classes = n_classes, \\\n",
    "                            n_clusters_per_class = 3, n_informative = 4, random_state = 0)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "y_score = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outcome(act, pred):\n",
    "    if act == pred:\n",
    "        return 'TP' if act == 1 else 'TN'\n",
    "    else:\n",
    "        return 'FN' if act == 1 else 'FP'\n",
    "        \n",
    "def calculate_roc_metrics(y_test, y_score, threshold, pos_label = 1):\n",
    "    y_pred = (y_score[:, pos_label] > threshold) * 1\n",
    "    outcomes = np.array(list(map(lambda x, y: find_outcome(x, y), y_test, y_pred)))\n",
    "    \n",
    "    tp = sum(outcomes=='TP')\n",
    "    tn = sum(outcomes=='TN')\n",
    "    fp = sum(outcomes=='FP')\n",
    "    fn = sum(outcomes=='FN')\n",
    "    \n",
    "    tpr = 0 if tp == 0 else tp/(tp+fn)\n",
    "    fpr = 0 if fp == 0 else fp/(fp+tn)        \n",
    "    return fpr, tpr     \n",
    "\n",
    "def get_auc_score(y, y_score):\n",
    "    n_datapoints = 100\n",
    "    thresholds = np.linspace(0, 1, n_datapoints)\n",
    "    roc_metrics = np.array(list(map(lambda x: calculate_roc_metrics(y, y_score, x), thresholds)))\n",
    "    fpr = roc_metrics[:,0]\n",
    "    tpr = roc_metrics[:,1]\n",
    "    return auc(fpr, tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_rest(y, y_score, n_classes):\n",
    "    AUC = []\n",
    "    for class_label in range(n_classes):\n",
    "        iter_y = np.where(y == class_label, 0, 1)\n",
    "        iter_y_score = np.vstack((y_score[:, class_label], 1 - y_score[:, class_label])).transpose()\n",
    "        auc_score = get_auc_score(iter_y, iter_y_score)\n",
    "        AUC += [auc_score]\n",
    "    return np.mean(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_one(y, n_classes):\n",
    "    AUC = []\n",
    "    combs = list(combinations(range(n_classes), 2))\n",
    "\n",
    "    for neg_class_label, pos_class_label in combs:\n",
    "        iter_X = X[(y == neg_class_label) | (y == pos_class_label)]\n",
    "        iter_y = y[(y == neg_class_label) | (y == pos_class_label)]\n",
    "        iter_y = np.where(iter_y == neg_class_label, 0, 1)\n",
    "\n",
    "        clf.fit(iter_X, iter_y)\n",
    "        iter_y_score = clf.predict_proba(iter_X)\n",
    "        \n",
    "        auc_score = get_auc_score(iter_y, iter_y_score)\n",
    "        AUC += [auc_score]\n",
    "    return np.mean(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8421645172434821, 0.8421976865259726)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_vs_rest(y, y_score, n_classes), roc_auc_score(y, y_score, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8679561547970424, 0.8420581530079008)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_vs_one(y, n_classes), roc_auc_score(y, y_score, multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "951daa5e1959839fcb325fff331f52e72634f7a1be998f6081ed7f433b63f1b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
