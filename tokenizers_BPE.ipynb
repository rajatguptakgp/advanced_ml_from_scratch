{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "nltk.download('brown')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "Tokenizers can be of different methods:\n",
    "1. **Word Tokenization:**\n",
    "    1. **Space Tokenization:** Breaking down a sentence into tokens based on white space between words.\n",
    "    2. **Punctuation Tokenization:** Tokenizing punctuations separately so that there aren't extra tokens for each combination of {word}_{punc}.\n",
    "    3. **Rule-based tokenization:** Tokenizing the word **Don't** can have an issue with Punctuation Tokenization, so adding extra rules.\n",
    "2. **Character Tokenization:**\n",
    "    1. Tokenizing each character and punctuation, small vocabulary size.\n",
    "    2. Trouble with context independent semantic representation. For example - representation of 'c' v/s representation of word 'car'\n",
    "3. **Subword-Tokenization:**\n",
    "    1. Best of both above - lesser vocabulary size, context independent semantic representations\n",
    "    2. Methods:\n",
    "        1. **BPE:** Byte-Pair Encoding, \n",
    "        2. **Byte-Level BPE:** Used in GPT\n",
    "        3. **Word-Piece:** Used in BERT\n",
    "        4. **Sentence-Piece:** Used in XLNet, T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Example - Character Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_occuring_pair(string, pairs):\n",
    "    max_count = 0\n",
    "    best_pair = None\n",
    "    for pair in pairs:\n",
    "        count = 0\n",
    "        for char_idx in range(len(string)):\n",
    "            if pair == string[char_idx:char_idx+2]:\n",
    "                count += 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            best_pair = pair\n",
    "    return best_pair\n",
    "\n",
    "\n",
    "def iteration(string, replacer):\n",
    "    vocab = list(set(string))\n",
    "\n",
    "    pairs1 = list(itertools.permutations(vocab, 2))\n",
    "    pairs2 = list(itertools.combinations_with_replacement(vocab, 2))\n",
    "    pairs = set(pairs1 + pairs2)\n",
    "    \n",
    "    pairs = list(map(lambda x: ''.join(x), pairs))\n",
    "    best_pair = find_most_occuring_pair(string, pairs)\n",
    "\n",
    "    if best_pair is not None:\n",
    "        string = string.replace(best_pair, replacer)\n",
    "    return string, best_pair\n",
    "    \n",
    "\n",
    "def run_BPE(input_string, replacers):\n",
    "    string = input_string\n",
    "    best_pair = input_string[:2]\n",
    "    vocab = {}\n",
    "    idx = 0 \n",
    "\n",
    "    while True:\n",
    "        string, best_pair = iteration(string, replacers[idx])\n",
    "        if best_pair is not None:\n",
    "            vocab[replacers[idx]] = best_pair\n",
    "            print('Best Pair:', best_pair, '| String:', string)\n",
    "            idx += 1\n",
    "        else:\n",
    "            return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String: aaabdaaabac \n",
      "\n",
      "Best Pair: aa | String: ZabdZabac\n",
      "Best Pair: Za | String: YbdYbac\n",
      "Best Pair: Yb | String: XdXac\n",
      "Best Pair: dX | String: XWac\n",
      "Best Pair: Wa | String: XVc\n",
      "Best Pair: XV | String: Uc\n",
      "Best Pair: Uc | String: T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Z': 'aa', 'Y': 'Za', 'X': 'Yb', 'W': 'dX', 'V': 'Wa', 'U': 'XV', 'T': 'Uc'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'aaabdaaabac'\n",
    "replacers = [chr(65 + i) for i in range(26)]\n",
    "replacers.reverse()\n",
    "\n",
    "print('Original String:', string, '\\n')\n",
    "run_BPE(string, replacers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face example - Word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pair(corpus, pairs):\n",
    "    max_count = 0\n",
    "    best_pair = None\n",
    "\n",
    "    for pair in pairs:\n",
    "        count = 0    \n",
    "        for idx in range(len(corpus)):\n",
    "            if pair in corpus[idx,0]:\n",
    "                count += int(corpus[idx,1])\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            best_pair = pair\n",
    "            \n",
    "    return max_count, best_pair\n",
    "\n",
    "\n",
    "def run_iteration(corpus):\n",
    "    chars = list(map(lambda x: list(x), corpus[:, 0]))\n",
    "    base_vocab = list(set(itertools.chain(*chars)))\n",
    "\n",
    "    pairs1 = list(itertools.permutations(base_vocab, 2))\n",
    "    pairs2 = list(itertools.combinations_with_replacement(base_vocab, 2))\n",
    "    pairs = set(pairs1 + pairs2)\n",
    "\n",
    "    pairs = list(map(lambda x: ''.join(x), pairs))\n",
    "\n",
    "    max_count, best_pair = find_best_pair(corpus, pairs)\n",
    "    return best_pair, len(base_vocab)\n",
    "\n",
    "\n",
    "def run_BPE(base_corpus, replacers, max_vocabulary_size):\n",
    "    corpus = base_corpus \n",
    "    idx = 0\n",
    "    vocabulary_size = float('inf')\n",
    "    while vocabulary_size >= max_vocabulary_size:\n",
    "        replacer = replacers[idx]\n",
    "        best_pair, vocabulary_size = run_iteration(corpus) \n",
    "        \n",
    "        if best_pair is not None:\n",
    "            corpus[:,0] = list(map(lambda x: x.replace(best_pair, replacer), corpus[:, 0]))\n",
    "            \n",
    "            print_corpus = list(map(lambda x: list([x[0], int(x[1])]), corpus))\n",
    "            print('Best Pair:', best_pair, '| Vocabulary Size:', vocabulary_size, '| Corpus:', print_corpus)\n",
    "            idx += 1\n",
    "        else:\n",
    "            return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Corpus: [('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)] \n",
      "\n",
      "Best Pair: ug | Vocabulary Size: 7 | Corpus: [['hZ', 10], ['pZ', 5], ['pun', 12], ['bun', 4], ['hZs', 5]]\n",
      "Best Pair: un | Vocabulary Size: 7 | Corpus: [['hZ', 10], ['pZ', 5], ['pY', 12], ['bY', 4], ['hZs', 5]]\n",
      "Best Pair: hZ | Vocabulary Size: 6 | Corpus: [['X', 10], ['pZ', 5], ['pY', 12], ['bY', 4], ['Xs', 5]]\n",
      "Best Pair: pY | Vocabulary Size: 6 | Corpus: [['X', 10], ['pZ', 5], ['W', 12], ['bY', 4], ['Xs', 5]]\n",
      "Best Pair: pZ | Vocabulary Size: 7 | Corpus: [['X', 10], ['V', 5], ['W', 12], ['bY', 4], ['Xs', 5]]\n",
      "Best Pair: Xs | Vocabulary Size: 6 | Corpus: [['X', 10], ['V', 5], ['W', 12], ['bY', 4], ['U', 5]]\n",
      "Best Pair: bY | Vocabulary Size: 6 | Corpus: [['X', 10], ['V', 5], ['W', 12], ['T', 4], ['U', 5]]\n"
     ]
    }
   ],
   "source": [
    "max_vocabulary_size = 6\n",
    "base_corpus = [('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)]\n",
    "print('Base Corpus:', base_corpus, '\\n')\n",
    "\n",
    "base_corpus = np.array(base_corpus)\n",
    "run_BPE(base_corpus, replacers, max_vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Piece Tokenizer\n",
    "\n",
    "Word Piece Tokenizer is sub-word tokenization scheme, similar to BPE and is used by BERT. We will briefly look into pre-trained word embeddings from BERT model, and how to prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 30522)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = bert.config.to_dict()['hidden_size']\n",
    "vocab_size = bert.config.to_dict()['vocab_size']\n",
    "embedding_size, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "max_length = tokenizer.model_max_length\n",
    "token2int = tokenizer.vocab\n",
    "int2token = {v:k for k, v in token2int.items()}\n",
    "vocab_size, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>: None\n",
      "<EOS>: None\n",
      "<PAD>: 0\n",
      "<UNK>: 100\n",
      "<CLS>: 101\n",
      "<SEP>: 102\n",
      "<MASK>: 103\n"
     ]
    }
   ],
   "source": [
    "print('<BOS>:', tokenizer.bos_token_id)\n",
    "print('<EOS>:', tokenizer.eos_token_id)\n",
    "print('<PAD>:', tokenizer.pad_token_id)\n",
    "print('<UNK>:', tokenizer.unk_token_id)\n",
    "print('<CLS>:', tokenizer.cls_token_id)\n",
    "print('<SEP>:', tokenizer.sep_token_id)\n",
    "print('<MASK>:', tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  4633,  2003,  3835,  2651,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1045,  2572,  2667,  2000,  4553,  2367,  7957,  1997, 19204,\n",
       "         17629,  2015,  2013, 17662,  2227,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = ['The weather is nice today.', 'I am trying to learn different kinds of Tokenizers from Hugging Face.']\n",
    "\n",
    "# PyTorch tensors\n",
    "encoded_input = tokenizer(sents, padding=True, truncation=True, return_tensors='pt')\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Sentence: [CLS] the weather is nice today. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Decoded Sentence: [CLS] i am trying to learn different kinds of tokenizers from hugging face. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(sents)):\n",
    "    decoded_sent = tokenizer.decode(encoded_input['input_ids'][idx])\n",
    "    print('Decoded Sentence:', decoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'trying', 'to', 'learn', 'different', 'kinds', 'of', 'tokenizers', 'from', 'hugging', 'face.']\n",
      "['i', 'am', 'trying', 'to', 'learn', 'different', 'kinds', 'of', 'token', '##izer', '##s', 'from', 'hugging', 'face', '.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "print(sents[1].lower().split(' '))\n",
    "\n",
    "# word piece tokenization\n",
    "print(tokenizer.tokenize(sents[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings\n",
    "\n",
    "BERT is Bi-Directional Encoder Representations from Transformers. It's trained on two tasks: \n",
    "1. Masked Language Modelling: Predicting masked tokens in sentence from non-masked ones\n",
    "2. Sentence Classification: Given two sentences A and B, classify whether A follows B or not\n",
    "\n",
    "\n",
    "The output of BERT layer is:\n",
    "1. **Sequence output:** \n",
    "    1. The hidden state of last layer in stacked model, capturing the context of whole sentence. \n",
    "    2. The shape of this output is: [num_examples, max_length, embedding_size]\n",
    "2. **Pooled output:** \n",
    "    1. This can be said as the embedding of [CLS] token\n",
    "    2. The shape of this output is: [num_examples, embedding_size]\n",
    "\n",
    "\n",
    "For tasks like Sentiment Analysis, we can do the following:\n",
    "1. Embedding of [CLS] token from sequence output i.e. 1st embedding vector for each sentence can be used as sentence summary\n",
    "2. Embedding of [CLS] token from pooled output, this is better representation of sentence embedding, and is typically followed for sentence classification\n",
    "3. Mean pooling (Averaging) of embeddings of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence output shape: torch.Size([2, 17, 768])\n",
      "Pooled output shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "output = bert(encoded_input['input_ids'])\n",
    "print('Sequence output shape:', output['last_hidden_state'].shape)\n",
    "print('Pooled output shape:', output['pooler_output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "951daa5e1959839fcb325fff331f52e72634f7a1be998f6081ed7f433b63f1b3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
